# -*- coding: utf-8 -*-
"""part2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19wLIypsLmTtikaDWwMuiQ1ZiJ-6jjn16
"""

import numpy as np
import pandas as pd

from ucimlrepo import fetch_ucirepo

# fetch dataset
student_performance = fetch_ucirepo(id=320)

# data (as pandas dataframes)
X = student_performance.data.features
y = student_performance.data.targets

# metadata
print(student_performance.metadata)

# variable information
print(student_performance.variables)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import itertools
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import SGDRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score

df = pd.concat([X, y["G3"]], axis=1)
print("DF shape:", df.shape)

# Preprocessing
print("\nNull counts:\n", df.isnull().sum())

df = df.dropna()
dup_count = df.duplicated().sum()
print("Duplicate rows:", dup_count)
df = df.drop_duplicates()

print(" Cleaned DF shape:", df.shape)

# Split Features/Target
target_col = "G3"
X_df = df.drop(columns=[target_col])
y = df[target_col].values

# Identify categorical vs numeric columns
categorical_cols = X_df.select_dtypes(include=["object"]).columns.tolist()
numeric_cols = [c for c in X_df.columns if c not in categorical_cols]

# 80/20 train/test split
X_train_df, X_test_df, y_train, y_test = train_test_split(
    X_df, y, test_size=0.2, random_state=42
)

print(f"\nTrain set: {len(X_train_df)}")
print(f"Test set: {len(X_test_df)}")

# Scale numeric columns with StandardScaler
# Use OneHotEncoder for categorical columns
preprocess = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numeric_cols),
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_cols),
    ],
    remainder="drop"
)

eta0_list = [0.0005, 0.001, 0.005, 0.01]
max_iter_list = [1000, 3000, 6000, 10000]
learning_rate_schedules = ["constant"]

results = []
best = {"test_mse": float("inf"), "params": None, "model": None}


# Hyperparameter Tuning Loop
for eta0, max_iter, lr_sched in itertools.product(eta0_list, max_iter_list, learning_rate_schedules):
    # Create a fresh model each time
    trial_model = Pipeline(steps=[
        ("preprocess", preprocess),
        ("sgd", SGDRegressor(
            loss="squared_error",
            penalty=None,
            learning_rate=lr_sched,
            eta0=eta0,
            max_iter=max_iter,
            tol=1e-8, # stopping tolerance
        ))
    ])

    trial_model.fit(X_train_df, y_train)

    train_pred = trial_model.predict(X_train_df)
    test_pred  = trial_model.predict(X_test_df)

    train_mse = mean_squared_error(y_train, train_pred)
    test_mse  = mean_squared_error(y_test, test_pred)

    train_rmse = np.sqrt(train_mse)
    test_rmse  = np.sqrt(test_mse)

    train_mae = mean_absolute_error(y_train, train_pred)
    test_mae  = mean_absolute_error(y_test, test_pred)

    train_r2 = r2_score(y_train, train_pred)
    test_r2  = r2_score(y_test, test_pred)

    train_ev = explained_variance_score(y_train, train_pred)
    test_ev  = explained_variance_score(y_test, test_pred)

    results.append({
        "eta0": eta0,
        "max_iter": max_iter,
        "learning_rate_schedule": lr_sched,
        "train_mse": train_mse,
        "test_mse": test_mse,
        "train_rmse": train_rmse,
        "test_rmse": test_rmse,
        "train_mae": train_mae,
        "test_mae": test_mae,
        "train_r2": train_r2,
        "test_r2": test_r2,
        "train_explained_variance": train_ev,
        "test_explained_variance": test_ev,
    })

    if test_mse < best["test_mse"]:
        best["test_mse"] = test_mse
        best["params"] = (eta0, max_iter, lr_sched)
        best["model"] = trial_model

results_df_sgd = pd.DataFrame(results).sort_values("test_mse").reset_index(drop=True)
results_df_sgd.to_csv("sgd_trials_log.csv", index=False)
print("\nSaved hyperparameter trials log to: sgd_trials_log.csv")
print("Best params (eta0, max_iter, schedule):", best["params"])
print("Best test MSE:", best["test_mse"])

best_model = best["model"]
sgd_est = best_model.named_steps["sgd"]
best_train_pred = best_model.predict(X_train_df)
best_test_pred  = best_model.predict(X_test_df)

print("\n" + "="*60)
print("Best SGDRegressor Model")
print("="*60)


print("\nBest hyperparameters:", best["params"])

print("\nCoefficients info:")
print("Intercept:", sgd_est.intercept_)
print("Number of coefficients:", len(sgd_est.coef_))

print("\nTrain Metrics:")
print("MSE:", mean_squared_error(y_train, best_train_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_train, best_train_pred)))
print("MAE:", mean_absolute_error(y_train, best_train_pred))
print("R2:", r2_score(y_train, best_train_pred))
print("EV:", explained_variance_score(y_train, best_train_pred))

print("\nTest Metrics:")
print("MSE:", mean_squared_error(y_test, best_test_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, best_test_pred)))
print("MAE:", mean_absolute_error(y_test, best_test_pred))
print("R2:", r2_score(y_test, best_test_pred))
print("EV:", explained_variance_score(y_test, best_test_pred))

#Visualizing Plots

# Actual vs Predicted (Test)
plt.figure()
plt.scatter(y_test, best_test_pred)
plt.xlabel("Actual G3")
plt.ylabel("Predicted G3")
plt.title("SGDRegressor: Actual vs Predicted (Test Set)")
plt.show()

# Residuals vs Predicted (Test)
residuals = y_test - best_test_pred
plt.figure()
plt.scatter(best_test_pred, residuals)
plt.xlabel("Predicted G3")
plt.ylabel("Residual (Actual - Predicted)")
plt.title("SGDRegressor: Residuals vs Predicted (Test Set)")
plt.axhline(0)
plt.show()

# Hyperparameter heatmap (Test MSE) for eta0 vs max_iter
heatmap_data = results_df_sgd.pivot(index="eta0", columns="max_iter", values="test_mse")

plt.figure(figsize=(8, 6))
sns.heatmap(heatmap_data, annot=True, fmt=".2f")
plt.xlabel("Max Iterations")
plt.ylabel("eta0 (Learning Rate)")
plt.title("SGDRegressor Hyperparameter Heatmap (Test MSE)")
plt.show()